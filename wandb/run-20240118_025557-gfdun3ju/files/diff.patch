diff --git a/FSDP_mnist.py b/FSDP_mnist.py
index eb2063f..9ac2a22 100644
--- a/FSDP_mnist.py
+++ b/FSDP_mnist.py
@@ -23,6 +23,7 @@ import numpy as np
 from torch.nn.parallel import DistributedDataParallel as DDP
 from torch.utils.data.distributed import DistributedSampler
 from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
+from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW
 from torch.distributed.fsdp.fully_sharded_data_parallel import (
 CPUOffload,
 BackwardPrefetch,
@@ -43,65 +44,24 @@ def setup(rank, world_size):
 
 def cleanup():
     dist.destroy_process_group()
-
-class QuestionEmbedding(nn.Module):
-    """
-    A question embedding module using LSTM for text encoding.
-    """
-    def __init__(self, word_embedding_size, hidden_size):
-        super(QuestionEmbedding, self).__init__()
-        self.lstm = nn.LSTM(word_embedding_size, hidden_size, num_layers=1, batch_first=True)
-        self.fc = nn.Linear(hidden_size, 1024)
-
-    def forward(self, input_data):
-        output, (hidden, _) = self.lstm(input_data)
-        last_hidden = hidden[-1]  # Get the last layer's hidden state
-        embedding = self.fc(last_hidden)
-        return embedding
-
-class Net(nn.Module):
-    """
-    A Visual Question Answering (VQA) model
-    """
-    def __init__(self, args, question_vocab_size, pretrain_embedding):
-        super(Net, self).__init__()
-        self.word_embeddings = nn.Embedding(question_vocab_size, 300)
-        if args.use_glove:
-            print(pretrain_embedding)
-            self.word_embeddings.weight.data.copy_(torch.from_numpy(pretrain_embedding))
-        self.question_encoder = QuestionEmbedding(
-            word_embedding_size=300,
-            hidden_size=1024)
-    
-        self.qt_header = nn.Sequential(
-            nn.Linear(1024, 1000),
-            nn.Dropout(p=0.5),
-            nn.Tanh(),
-            nn.Linear(1000, 8)
-        )
-
-    def forward(self, question):
-        question = self.word_embeddings(question)
-        question = self.question_encoder(question)
-        output = self.qt_header(question)
-        return output
     
 def train(args, model, rank, world_size, train_loader, optimizer, epoch, sampler=None):
     model.train()
     ddp_loss = torch.zeros(2).to(rank)
     if sampler:
         sampler.set_epoch(epoch)
-    for batch_idx, (data, target, question_id) in enumerate(train_loader):
-        data, target = data.to(rank), target.to(rank)
+    for batch_idx, batch in enumerate(train_loader):
+        input_ids = batch['input_ids'].to(rank)
+        attention_mask = batch['attention_mask'].to(rank)
+        labels = batch['labels'].to(rank)
         
+        output = model(input_ids, attention_mask=attention_mask, labels=labels)
         optimizer.zero_grad()
-        output = model(data)
-        
-        loss = F.cross_entropy(output, target, reduction='sum')
+        loss = output.loss
         loss.backward()
         optimizer.step()
         ddp_loss[0] += loss.item()
-        ddp_loss[1] += len(data)
+        ddp_loss[1] += len(input_ids)
         # wandb.log({
         #     "iter_loss": loss.item()
         # })
@@ -121,18 +81,26 @@ def test(model, rank, world_size, test_loader, epoch):
     local_targets = []  # List to store targets
     ddp_loss = torch.zeros(3).to(rank)
     with torch.no_grad():
-        for data, target, question_id in test_loader:  # Assuming question_id is part of your dataloader
-            data, target = data.to(rank), target.to(rank)
-            output = model(data)
-            _, pred = torch.max(output, 1)
+        for batch in test_loader:  # Assuming question_id is part of your dataloader
+            question_id = batch['question_id'].to(rank)
+            input_ids = batch['input_ids'].to(rank)
+            attention_mask = batch['attention_mask'].to(rank)
+            labels = batch['labels'].to(rank)
+            
+            output = model(input_ids, attention_mask=attention_mask, labels=labels)
+            logits = output.logits
+            probabilities = F.softmax(logits, dim=1)
+            pred = torch.argmax(probabilities, dim=1)
+            loss = output.loss
+            
             local_preds.extend(pred.cpu().numpy().tolist())
             local_question_ids.extend(question_id.cpu().numpy().tolist())  # Gather question IDs
-            local_targets.extend(target.cpu().numpy().tolist())  # Gather targets
+            local_targets.extend(labels.cpu().numpy().tolist())  # Gather targets
 
             # Loss calculation
-            ddp_loss[0] += F.cross_entropy(output, target, reduction='sum').item()
-            ddp_loss[1] += pred.eq(target.view_as(pred)).sum().item()
-            ddp_loss[2] += len(data)
+            ddp_loss[0] += loss.item()
+            ddp_loss[1] += pred.eq(labels.view_as(pred)).sum().item()
+            ddp_loss[2] += len(input_ids)
 
     # Gather all data to rank 0
     gathered_data = []
@@ -172,10 +140,9 @@ def fsdp_main(rank, world_size, args):
     setup(rank, world_size)
     wandb.init(
             project="Question Type",
-            group="LSTM+GLOVE",
-            name= f"LSTM+GLOVE {rank}",
-            config=args,
-            dir="/home/ndhuynh/github/Question-Analysis/wandb"
+            group="RoBERT",
+            name= f"RoBERT {rank}",
+            config=args
             )
 
     dataset1 = QuestionDataset(args, "train")
@@ -201,19 +168,16 @@ def fsdp_main(rank, world_size, args):
 
     init_start_event = torch.cuda.Event(enable_timing=True)
     init_end_event = torch.cuda.Event(enable_timing=True)
-
-    model = Net(question_vocab_size = dataset1.token_size, pretrain_embedding = dataset1.pretrained_emb, args = args).to(rank)
-
-    # model = FSDP(model,
-    #             auto_wrap_policy=my_auto_wrap_policy,
-    #             cpu_offload=CPUOffload(offload_params=True))
+    
+    model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=dataset1.output_dim).to(rank)
     model = FSDP(model,
             auto_wrap_policy=my_auto_wrap_policy)
+    optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)
+    
+
     if rank == 0:
         print(f"{model}")
 
-    optimizer = optim.Adam(model.parameters(), lr=args.lr)
-
     scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)
     init_start_event.record()
     best_test_result = 0
@@ -291,7 +255,7 @@ def fsdp_main(rank, world_size, args):
 if __name__ == '__main__':
     # Training settings
     parser = argparse.ArgumentParser(description='PyTorch MNIST Example')
-    parser.add_argument('--batch-size', type=int, default=2048, metavar='N',
+    parser.add_argument('--batch-size', type=int, default=512, metavar='N',
                         help='input batch size for training (default: 64)')
     parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',
                         help='input batch size for testing (default: 1000)')
diff --git a/config.yaml b/config.yaml
index be5b978..c95da2b 100644
--- a/config.yaml
+++ b/config.yaml
@@ -4,6 +4,6 @@ stat_ques_list: ["/home/ndhuynh/data/simpsonsvqa/v1_Question_Train_simpsons_vqa.
 use_glove: True
 train_question: '/home/ndhuynh/data/simpsonsvqa/v1_Question_Train_simpsons_vqa.json'
 val_question: '/home/ndhuynh/data/simpsonsvqa/v1_Question_Val_simpsons_vqa.json'
-: 20
+max_ques_len: 20
 train_annotation: '/home/ndhuynh/data/simpsonsvqa/v1_Annotation_Train_simpsons_vqa.json'
 val_annotation : '/home/ndhuynh/data/simpsonsvqa/v1_Annotation_Val_simpsons_vqa.json'
\ No newline at end of file
diff --git a/dataset.py b/dataset.py
index 0d44d1b..34157b3 100644
--- a/dataset.py
+++ b/dataset.py
@@ -7,6 +7,7 @@ import en_core_web_lg, random, re, json
 import numpy as np
 import random
 import pandas as pd
+from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW
 contractions = {
     "aint": "ain't", "arent": "aren't", "cant": "can't", "couldve":
     "could've", "couldnt": "couldn't", "couldn'tve": "couldn't've",
@@ -151,21 +152,17 @@ class QuestionDataset(Dataset):
     def __init__(self, args, split):
         self.args = args
         self.split = split
-        self.token_to_ix, self.pretrained_emb = self.load_vocal()
-        self.token_size = len(self.token_to_ix)
-        with open('./super_answer_type_simpsons.json', 'r') as file:
+        with open('/home/ndhuynh/github/simpsonsvqa/dataset/super_answer_type_simpsons.json', 'r') as file:
             self.super_types = json.load(file)
         self.questions = self.load_questions()
-        
+        self.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
         self.ans_type_to_idx = {'yes/no': 0, 'action': 1, 'object': 2, 'location': 3, 'other': 4, 'color': 5, 'human': 6, 'number': 7}
         self.idx_to_ans_type = {0: 'yes/no', 1: 'action', 2: 'object', 3: 'location', 4: 'other', 5: 'color', 6: 'human', 7: 'number'}
-        
         self.annotations = self.load_annotations()
         self.output_dim = len(self.ans_type_to_idx.keys())
         random.shuffle(self.annotations)
         print(f"sample number: {len(self.annotations)}")
         print(f"output_dim: {self.output_dim }")
-        print(f"token_size: {self.token_size }")
         print(f"ans_type_to_idx: {self.ans_type_to_idx }")
     def __len__(self):
         return len(self.annotations)
@@ -174,15 +171,28 @@ class QuestionDataset(Dataset):
         ann = self.annotations[idx]
         que = self.questions[ann["id"]]
         question_id = ann["id"]
-        question = torch.from_numpy(que["question"])
+        question = que["question"]
         label = self.ans_type_to_idx[ann['answer_type']]
-        # print('-'*100)
-        # print(ann)
-        # print(label)
-        # return question_id, question, label
-        return  question, label, question_id
+        
+        encoding = self.tokenizer.encode_plus(
+            question,
+            add_special_tokens=True,
+            max_length=self.args.max_ques_len,
+            return_token_type_ids=False,
+            padding='max_length',
+            truncation=True,
+            return_attention_mask=True,
+            return_tensors='pt',
+        )
+        
+        return {
+            'question_id': question_id,
+            'question_text': question,
+            'input_ids': encoding['input_ids'].flatten(),
+            'attention_mask': encoding['attention_mask'].flatten(),
+            'labels': torch.tensor(label, dtype=torch.long)
+        }
     
-
     def load_questions(self):
         if self.split == "train":
             question_path = self.args.train_question
@@ -192,7 +202,6 @@ class QuestionDataset(Dataset):
             questions = json.load(file)["questions"]
         processed_questions = {}
         for question in questions:
-            question['question'] = rnn_proc_ques(question["question"], self.token_to_ix, self.args.max_ques_len)
             processed_questions[question["id"]] = question
         return processed_questions
     
