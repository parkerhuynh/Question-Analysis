diff --git a/FSDP_mnist.py b/FSDP_mnist.py
index eb2063f..787a03f 100644
--- a/FSDP_mnist.py
+++ b/FSDP_mnist.py
@@ -46,16 +46,19 @@ def cleanup():
 
 class QuestionEmbedding(nn.Module):
     """
-    A question embedding module using LSTM for text encoding.
+    A question embedding module using a two-layer BiLSTM for text encoding.
     """
     def __init__(self, word_embedding_size, hidden_size):
         super(QuestionEmbedding, self).__init__()
-        self.lstm = nn.LSTM(word_embedding_size, hidden_size, num_layers=1, batch_first=True)
-        self.fc = nn.Linear(hidden_size, 1024)
+        # Using a two-layer BiLSTM
+        self.lstm = nn.LSTM(word_embedding_size, hidden_size, num_layers=2, batch_first=True, bidirectional=True)
+        # The output feature dimension of BiLSTM is 2 * hidden_size
+        self.fc = nn.Linear(2 * hidden_size, 1024)
 
     def forward(self, input_data):
         output, (hidden, _) = self.lstm(input_data)
-        last_hidden = hidden[-1]  # Get the last layer's hidden state
+        # Concatenate the final hidden states of the forward and backward passes from the last layer
+        last_hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)
         embedding = self.fc(last_hidden)
         return embedding
 
@@ -74,7 +77,7 @@ class Net(nn.Module):
             hidden_size=1024)
     
         self.qt_header = nn.Sequential(
-            nn.Linear(1024, 1000),
+            nn.Linear(1024*2, 1000),
             nn.Dropout(p=0.5),
             nn.Tanh(),
             nn.Linear(1000, 8)
@@ -172,8 +175,8 @@ def fsdp_main(rank, world_size, args):
     setup(rank, world_size)
     wandb.init(
             project="Question Type",
-            group="LSTM+GLOVE",
-            name= f"LSTM+GLOVE {rank}",
+            group="BiLSTM+GLOVE",
+            name= f"BiLSTM+GLOVE {rank}",
             config=args,
             dir="/home/ndhuynh/github/Question-Analysis/wandb"
             )
diff --git a/config.yaml b/config.yaml
index be5b978..c95da2b 100644
--- a/config.yaml
+++ b/config.yaml
@@ -4,6 +4,6 @@ stat_ques_list: ["/home/ndhuynh/data/simpsonsvqa/v1_Question_Train_simpsons_vqa.
 use_glove: True
 train_question: '/home/ndhuynh/data/simpsonsvqa/v1_Question_Train_simpsons_vqa.json'
 val_question: '/home/ndhuynh/data/simpsonsvqa/v1_Question_Val_simpsons_vqa.json'
-: 20
+max_ques_len: 20
 train_annotation: '/home/ndhuynh/data/simpsonsvqa/v1_Annotation_Train_simpsons_vqa.json'
 val_annotation : '/home/ndhuynh/data/simpsonsvqa/v1_Annotation_Val_simpsons_vqa.json'
\ No newline at end of file
