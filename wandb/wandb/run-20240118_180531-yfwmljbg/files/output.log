sample number: 96169
output_dim: 8
token_size: 7057
ans_type_to_idx: {'yes/no': 0, 'action': 1, 'object': 2, 'location': 3, 'other': 4, 'color': 5, 'human': 6, 'number': 7}
sample number: 18473
output_dim: 8
token_size: 7057
ans_type_to_idx: {'yes/no': 0, 'action': 1, 'object': 2, 'location': 3, 'other': 4, 'color': 5, 'human': 6, 'number': 7}
[[ 0.48409   0.33505   0.42066  ... -1.7047   -0.62474   0.69989 ]
 [ 0.14354   0.057574  0.070036 ... -0.057567 -0.037463  0.060437]
 [-7.1395   -2.4339   -2.5181   ... -4.8147   -5.1779    6.7697  ]
 ...
 [ 3.9665   -0.4368   -4.9797   ...  1.2008   -2.9568    2.8831  ]
 [-1.0742    0.28313  -1.0317   ...  2.0254   -1.347     0.38743 ]
 [ 1.2023   -1.0608   -0.7661   ...  2.0222    2.6343    3.7644  ]]
FullyShardedDataParallel(
  (_fsdp_wrapped_module): FlattenParamsWrapper(
    (_fpw_module): Net(
      (word_embeddings): FullyShardedDataParallel(
        (_fsdp_wrapped_module): FlattenParamsWrapper(
          (_fpw_module): Embedding(7057, 300)
        )
      )
      (question_encoder): QuestionEmbedding(
        (lstm): FullyShardedDataParallel(
          (_fsdp_wrapped_module): FlattenParamsWrapper(
            (_fpw_module): LSTM(300, 1024, num_layers=2, batch_first=True, bidirectional=True)
          )
        )
        (fc): FullyShardedDataParallel(
          (_fsdp_wrapped_module): FlattenParamsWrapper(
            (_fpw_module): Linear(in_features=2048, out_features=1024, bias=True)
          )
        )
      )
      (qt_header): Sequential(
        (0): FullyShardedDataParallel(
          (_fsdp_wrapped_module): FlattenParamsWrapper(
            (_fpw_module): Linear(in_features=1024, out_features=1000, bias=True)
          )
        )
        (1): Dropout(p=0.5, inplace=False)
        (2): Tanh()
        (3): Linear(in_features=1000, out_features=8, bias=True)
      )
    )
  )
)
/home/ndhuynh/.conda/envs/hie/lib/python3.9/site-packages/torch/nn/modules/rnn.py:769: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484775609/work/aten/src/ATen/native/cudnn/RNN.cpp:968.)
  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
Train Epoch: 1 	Loss: 1.955527
Test set: Average loss: 1.8095, Accuracy: 8653/18474 (46.84%)
Train Epoch: 2 	Loss: 1.627469
Test set: Average loss: 1.4849, Accuracy: 9619/18474 (52.07%)
Train Epoch: 3 	Loss: 1.295238
Test set: Average loss: 1.1009, Accuracy: 11885/18474 (64.33%)
Train Epoch: 4 	Loss: 0.916659
Test set: Average loss: 0.7630, Accuracy: 14957/18474 (80.96%)
Train Epoch: 5 	Loss: 0.654884
Test set: Average loss: 0.6008, Accuracy: 15222/18474 (82.40%)
Train Epoch: 6 	Loss: 0.533013
Test set: Average loss: 0.5150, Accuracy: 15445/18474 (83.60%)
Train Epoch: 7 	Loss: 0.452610
Test set: Average loss: 0.4414, Accuracy: 16261/18474 (88.02%)
Train Epoch: 8 	Loss: 0.378771
Test set: Average loss: 0.3744, Accuracy: 16548/18474 (89.57%)
Train Epoch: 9 	Loss: 0.324632
Test set: Average loss: 0.3348, Accuracy: 16650/18474 (90.13%)
Train Epoch: 10 	Loss: 0.291631
Test set: Average loss: 0.3153, Accuracy: 16734/18474 (90.58%)
Train Epoch: 11 	Loss: 0.275984
Test set: Average loss: 0.3123, Accuracy: 16770/18474 (90.78%)
Train Epoch: 12 	Loss: 0.266305
Test set: Average loss: 0.2968, Accuracy: 16848/18474 (91.20%)
Train Epoch: 13 	Loss: 0.253471
Test set: Average loss: 0.2970, Accuracy: 16894/18474 (91.45%)
Train Epoch: 14 	Loss: 0.244293
Test set: Average loss: 0.2782, Accuracy: 16928/18474 (91.63%)
Train Epoch: 15 	Loss: 0.239743
Test set: Average loss: 0.2716, Accuracy: 16963/18474 (91.82%)
Train Epoch: 16 	Loss: 0.231058
Test set: Average loss: 0.2682, Accuracy: 17021/18474 (92.13%)
Train Epoch: 17 	Loss: 0.225669
Test set: Average loss: 0.2788, Accuracy: 17025/18474 (92.16%)
Train Epoch: 18 	Loss: 0.224055
Test set: Average loss: 0.2601, Accuracy: 17061/18474 (92.35%)
Train Epoch: 19 	Loss: 0.217234
Test set: Average loss: 0.2570, Accuracy: 17071/18474 (92.41%)
Train Epoch: 20 	Loss: 0.210439
Test set: Average loss: 0.2535, Accuracy: 17101/18474 (92.57%)
Train Epoch: 21 	Loss: 0.207461
Test set: Average loss: 0.2538, Accuracy: 17082/18474 (92.47%)
Train Epoch: 22 	Loss: 0.202672
Test set: Average loss: 0.2508, Accuracy: 17110/18474 (92.62%)
Train Epoch: 23 	Loss: 0.200507
Test set: Average loss: 0.2519, Accuracy: 17122/18474 (92.68%)
Train Epoch: 24 	Loss: 0.198286
Test set: Average loss: 0.2494, Accuracy: 17116/18474 (92.65%)
Train Epoch: 25 	Loss: 0.194554
Test set: Average loss: 0.2460, Accuracy: 17174/18474 (92.96%)
Train Epoch: 26 	Loss: 0.191666
Test set: Average loss: 0.2601, Accuracy: 17139/18474 (92.77%)
Train Epoch: 27 	Loss: 0.189794
Test set: Average loss: 0.2381, Accuracy: 17217/18474 (93.20%)
Train Epoch: 28 	Loss: 0.186090
Test set: Average loss: 0.2393, Accuracy: 17228/18474 (93.26%)
Train Epoch: 29 	Loss: 0.184856
Test set: Average loss: 0.2365, Accuracy: 17241/18474 (93.33%)
Train Epoch: 30 	Loss: 0.182340
Test set: Average loss: 0.2374, Accuracy: 17243/18474 (93.34%)
Train Epoch: 31 	Loss: 0.179943
Test set: Average loss: 0.2346, Accuracy: 17175/18474 (92.97%)
Train Epoch: 32 	Loss: 0.177729
Test set: Average loss: 0.2311, Accuracy: 17241/18474 (93.33%)
Train Epoch: 33 	Loss: 0.178701
Test set: Average loss: 0.2324, Accuracy: 17188/18474 (93.04%)
Train Epoch: 34 	Loss: 0.174295
Test set: Average loss: 0.2295, Accuracy: 17274/18474 (93.50%)
Train Epoch: 35 	Loss: 0.170890
Test set: Average loss: 0.2355, Accuracy: 17267/18474 (93.47%)
Train Epoch: 36 	Loss: 0.168055
Test set: Average loss: 0.2351, Accuracy: 17267/18474 (93.47%)
Train Epoch: 37 	Loss: 0.167579
Test set: Average loss: 0.2312, Accuracy: 17285/18474 (93.56%)
Train Epoch: 38 	Loss: 0.166111
Test set: Average loss: 0.2327, Accuracy: 17219/18474 (93.21%)
Train Epoch: 39 	Loss: 0.171445
Test set: Average loss: 0.2544, Accuracy: 17165/18474 (92.91%)
Train Epoch: 40 	Loss: 0.170074
Test set: Average loss: 0.2309, Accuracy: 17292/18474 (93.60%)
Train Epoch: 41 	Loss: 0.162598
Test set: Average loss: 0.2299, Accuracy: 17292/18474 (93.60%)
Train Epoch: 42 	Loss: 0.161198
Test set: Average loss: 0.2247, Accuracy: 17310/18474 (93.70%)
Train Epoch: 43 	Loss: 0.158339
Test set: Average loss: 0.2281, Accuracy: 17306/18474 (93.68%)
Train Epoch: 44 	Loss: 0.156045
Test set: Average loss: 0.2363, Accuracy: 17303/18474 (93.66%)
Train Epoch: 45 	Loss: 0.155170
Test set: Average loss: 0.2237, Accuracy: 17287/18474 (93.57%)
Train Epoch: 46 	Loss: 0.156373
Test set: Average loss: 0.2284, Accuracy: 17278/18474 (93.53%)
Train Epoch: 47 	Loss: 0.154197
Test set: Average loss: 0.2288, Accuracy: 17266/18474 (93.46%)
Train Epoch: 48 	Loss: 0.152436
Test set: Average loss: 0.2298, Accuracy: 17288/18474 (93.58%)
Train Epoch: 49 	Loss: 0.149927
Test set: Average loss: 0.2284, Accuracy: 17325/18474 (93.78%)
Train Epoch: 50 	Loss: 0.151768
Test set: Average loss: 0.2223, Accuracy: 17292/18474 (93.60%)
CUDA event elapsed time: 10771.451sec
FullyShardedDataParallel(
  (_fsdp_wrapped_module): FlattenParamsWrapper(
    (_fpw_module): Net(
      (word_embeddings): FullyShardedDataParallel(
        (_fsdp_wrapped_module): FlattenParamsWrapper(
          (_fpw_module): Embedding(7057, 300)
        )
      )
      (question_encoder): QuestionEmbedding(
        (lstm): FullyShardedDataParallel(
          (_fsdp_wrapped_module): FlattenParamsWrapper(
            (_fpw_module): LSTM(300, 1024, num_layers=2, batch_first=True, bidirectional=True)
          )
        )
        (fc): FullyShardedDataParallel(
          (_fsdp_wrapped_module): FlattenParamsWrapper(
            (_fpw_module): Linear(in_features=2048, out_features=1024, bias=True)
          )
        )
      )
      (qt_header): Sequential(
        (0): FullyShardedDataParallel(
          (_fsdp_wrapped_module): FlattenParamsWrapper(
            (_fpw_module): Linear(in_features=1024, out_features=1000, bias=True)
          )
        )
        (1): Dropout(p=0.5, inplace=False)
        (2): Tanh()
        (3): Linear(in_features=1000, out_features=8, bias=True)
      )
    )
  )
)