diff --git a/FSDP_mnist.py b/FSDP_mnist.py
index eb2063f..71a3517 100644
--- a/FSDP_mnist.py
+++ b/FSDP_mnist.py
@@ -46,16 +46,19 @@ def cleanup():
 
 class QuestionEmbedding(nn.Module):
     """
-    A question embedding module using LSTM for text encoding.
+    A question embedding module using a two-layer BiLSTM for text encoding.
     """
     def __init__(self, word_embedding_size, hidden_size):
         super(QuestionEmbedding, self).__init__()
-        self.lstm = nn.LSTM(word_embedding_size, hidden_size, num_layers=1, batch_first=True)
-        self.fc = nn.Linear(hidden_size, 1024)
+        # Using a two-layer BiLSTM
+        self.lstm = nn.LSTM(word_embedding_size, hidden_size, num_layers=2, batch_first=True, bidirectional=True)
+        # The output feature dimension of BiLSTM is 2 * hidden_size
+        self.fc = nn.Linear(2 * hidden_size, 1024)
 
     def forward(self, input_data):
         output, (hidden, _) = self.lstm(input_data)
-        last_hidden = hidden[-1]  # Get the last layer's hidden state
+        # Concatenate the final hidden states of the forward and backward passes from the last layer
+        last_hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)
         embedding = self.fc(last_hidden)
         return embedding
 
@@ -122,7 +125,7 @@ def test(model, rank, world_size, test_loader, epoch):
     ddp_loss = torch.zeros(3).to(rank)
     with torch.no_grad():
         for data, target, question_id in test_loader:  # Assuming question_id is part of your dataloader
-            data, target = data.to(rank), target.to(rank)
+            data, target, question_id = data.to(rank), target.to(rank), question_id.to(rank)
             output = model(data)
             _, pred = torch.max(output, 1)
             local_preds.extend(pred.cpu().numpy().tolist())
@@ -172,8 +175,8 @@ def fsdp_main(rank, world_size, args):
     setup(rank, world_size)
     wandb.init(
             project="Question Type",
-            group="LSTM+GLOVE",
-            name= f"LSTM+GLOVE {rank}",
+            group="BiLSTM",
+            name= f"BiLSTM {rank}",
             config=args,
             dir="/home/ndhuynh/github/Question-Analysis/wandb"
             )
@@ -225,8 +228,7 @@ def fsdp_main(rank, world_size, args):
         if rank == 0:
             if test_acc > best_test_result:
                 best_test_result = test_acc
-                test_result["prediction"] =test_result["prediction"].map(dataset1.idx_to_ans_type)
-                test_result["target"] =test_result["target"].map(dataset1.idx_to_ans_type)
+                
                 wandb.log({"best_accuracy": best_test_result})
                 if args.save_model:
                     # use a barrier to make sure training is done on all ranks
@@ -238,6 +240,8 @@ def fsdp_main(rank, world_size, args):
     init_end_event.record()
 
     if rank == 0:
+        test_result["prediction"] =test_result["prediction"].map(dataset1.idx_to_ans_type)
+        test_result["target"] =test_result["target"].map(dataset1.idx_to_ans_type)
         test_result.to_csv("predictions.csv", index=False)
         print(f"CUDA event elapsed time: {init_start_event.elapsed_time(init_end_event) / 1000}sec")
         print(f"{model}")
@@ -295,7 +299,7 @@ if __name__ == '__main__':
                         help='input batch size for training (default: 64)')
     parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',
                         help='input batch size for testing (default: 1000)')
-    parser.add_argument('--epochs', type=int, default=50, metavar='N',
+    parser.add_argument('--epochs', type=int, default=1, metavar='N',
                         help='number of epochs to train (default: 14)')
     parser.add_argument('--lr', type=float, default=0.00001, metavar='LR',
                         help='learning rate (default: 1.0)')
diff --git a/config.yaml b/config.yaml
index be5b978..6f60dea 100644
--- a/config.yaml
+++ b/config.yaml
@@ -1,9 +1,9 @@
 ## Training:
 question_path: "/home/ndhuynh/data/simpsonsvqa/v1_Question_Train_simpsons_vqa.json"
 stat_ques_list: ["/home/ndhuynh/data/simpsonsvqa/v1_Question_Train_simpsons_vqa.json","/home/ndhuynh/data/simpsonsvqa/v1_Question_Val_simpsons_vqa.json"]
-use_glove: True
+use_glove: False
 train_question: '/home/ndhuynh/data/simpsonsvqa/v1_Question_Train_simpsons_vqa.json'
 val_question: '/home/ndhuynh/data/simpsonsvqa/v1_Question_Val_simpsons_vqa.json'
-: 20
+max_ques_len: 20
 train_annotation: '/home/ndhuynh/data/simpsonsvqa/v1_Annotation_Train_simpsons_vqa.json'
 val_annotation : '/home/ndhuynh/data/simpsonsvqa/v1_Annotation_Val_simpsons_vqa.json'
\ No newline at end of file
diff --git a/dataset.py b/dataset.py
index 0d44d1b..b65a4ef 100644
--- a/dataset.py
+++ b/dataset.py
@@ -153,7 +153,7 @@ class QuestionDataset(Dataset):
         self.split = split
         self.token_to_ix, self.pretrained_emb = self.load_vocal()
         self.token_size = len(self.token_to_ix)
-        with open('./super_answer_type_simpsons.json', 'r') as file:
+        with open('super_answer_type_simpsons.json', 'r') as file:
             self.super_types = json.load(file)
         self.questions = self.load_questions()
         
@@ -176,13 +176,18 @@ class QuestionDataset(Dataset):
         question_id = ann["id"]
         question = torch.from_numpy(que["question"])
         label = self.ans_type_to_idx[ann['answer_type']]
-        # print('-'*100)
-        # print(ann)
-        # print(label)
-        # return question_id, question, label
         return  question, label, question_id
     
-
+    def load_vocal(self):
+        stat_ques_list = []
+        for file_path in self.args.stat_ques_list:
+            with open(file_path, 'r') as file:
+                question_i = json.load(file)["questions"]
+                stat_ques_list += question_i
+        token_to_ix, pretrained_emb = LSTM_tokenize(stat_ques_list, self.args)
+        # pickle.dump([token_to_ix, pretrained_emb], open("./question_dict.pkl", 'wb'))
+        return token_to_ix, pretrained_emb
+    
     def load_questions(self):
         if self.split == "train":
             question_path = self.args.train_question
