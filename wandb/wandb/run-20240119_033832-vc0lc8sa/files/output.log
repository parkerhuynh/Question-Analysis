sample number: 96169
output_dim: 8
token_size: 7057
ans_type_to_idx: {'yes/no': 0, 'action': 1, 'object': 2, 'location': 3, 'other': 4, 'color': 5, 'human': 6, 'number': 7}
sample number: 18473
output_dim: 8
token_size: 7057
ans_type_to_idx: {'yes/no': 0, 'action': 1, 'object': 2, 'location': 3, 'other': 4, 'color': 5, 'human': 6, 'number': 7}
FullyShardedDataParallel(
  (_fsdp_wrapped_module): FlattenParamsWrapper(
    (_fpw_module): Net(
      (word_embeddings): FullyShardedDataParallel(
        (_fsdp_wrapped_module): FlattenParamsWrapper(
          (_fpw_module): Embedding(7057, 300)
        )
      )
      (question_encoder): QuestionEmbedding(
        (lstm): FullyShardedDataParallel(
          (_fsdp_wrapped_module): FlattenParamsWrapper(
            (_fpw_module): LSTM(300, 1024, num_layers=2, batch_first=True, bidirectional=True)
          )
        )
        (fc): FullyShardedDataParallel(
          (_fsdp_wrapped_module): FlattenParamsWrapper(
            (_fpw_module): Linear(in_features=2048, out_features=1024, bias=True)
          )
        )
      )
      (qt_header): Sequential(
        (0): FullyShardedDataParallel(
          (_fsdp_wrapped_module): FlattenParamsWrapper(
            (_fpw_module): Linear(in_features=1024, out_features=1000, bias=True)
          )
        )
        (1): Dropout(p=0.5, inplace=False)
        (2): Tanh()
        (3): Linear(in_features=1000, out_features=8, bias=True)
      )
    )
  )
)
/home/ndhuynh/.conda/envs/hie/lib/python3.9/site-packages/torch/nn/modules/rnn.py:769: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484775609/work/aten/src/ATen/native/cudnn/RNN.cpp:968.)
  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
Train Epoch: 1 	Loss: 2.005729
Test set: Average loss: 1.9024, Accuracy: 6903/18474 (37.37%)
CUDA event elapsed time: 193.535140625sec
FullyShardedDataParallel(
  (_fsdp_wrapped_module): FlattenParamsWrapper(
    (_fpw_module): Net(
      (word_embeddings): FullyShardedDataParallel(
        (_fsdp_wrapped_module): FlattenParamsWrapper(
          (_fpw_module): Embedding(7057, 300)
        )
      )
      (question_encoder): QuestionEmbedding(
        (lstm): FullyShardedDataParallel(
          (_fsdp_wrapped_module): FlattenParamsWrapper(
            (_fpw_module): LSTM(300, 1024, num_layers=2, batch_first=True, bidirectional=True)
          )
        )
        (fc): FullyShardedDataParallel(
          (_fsdp_wrapped_module): FlattenParamsWrapper(
            (_fpw_module): Linear(in_features=2048, out_features=1024, bias=True)
          )
        )
      )
      (qt_header): Sequential(
        (0): FullyShardedDataParallel(
          (_fsdp_wrapped_module): FlattenParamsWrapper(
            (_fpw_module): Linear(in_features=1024, out_features=1000, bias=True)
          )
        )
        (1): Dropout(p=0.5, inplace=False)
        (2): Tanh()
        (3): Linear(in_features=1000, out_features=8, bias=True)
      )
    )
  )
)